{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Pre-assignment\n",
    " - After succefully loading data from Amazon RDS to Hadoop using Sqoop\n",
    " - Tranforming data into dimension and fact tables\n",
    " - And, store it into Amazon S3 bucket using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages and declare variables\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pyspark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType, LongType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-0-98.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f58d420c990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Spark session\n",
    "spark = SparkSession.builder.appName('demo').master(\"local\").enableHiveSupport().getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from hadoop\n",
    "df = spark.read.load(\"/user/root/ETL/SRC_ATM_TRANS\", format=\"csv\", sep=\",\", inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking column size\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: double (nullable = true)\n",
      " |-- _c13: double (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: double (nullable = true)\n",
      " |-- _c21: double (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: double (nullable = true)\n",
      " |-- _c25: integer (nullable = true)\n",
      " |-- _c26: integer (nullable = true)\n",
      " |-- _c27: integer (nullable = true)\n",
      " |-- _c28: integer (nullable = true)\n",
      " |-- _c29: double (nullable = true)\n",
      " |-- _c30: integer (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: string (nullable = true)\n",
      " |-- _c33: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+------+---+--------+---+---+----------+-------------------+----+----+------+------+----+----------+----+----------+----+----+------+------+-------+--------------+------+----+----+----+----+-----+----+----+-------+--------------------+\n",
      "| _c0|    _c1|_c2|   _c3|_c4|     _c5|_c6|_c7|       _c8|                _c9|_c10|_c11|  _c12|  _c13|_c14|      _c15|_c16|      _c17|_c18|_c19|  _c20|  _c21|   _c22|          _c23|  _c24|_c25|_c26|_c27|_c28| _c29|_c30|_c31|   _c32|                _c33|\n",
      "+----+-------+---+------+---+--------+---+---+----------+-------------------+----+----+------+------+----+----------+----+----------+----+----+------+------+-------+--------------+------+----+----+----+----+-----+----+----+-------+--------------------+\n",
      "|2017|January|  1|Sunday|  0|  Active|  1|NCR|NÃƒÂ¦stved|        Farimagsvej|   8|4700|55.233|11.763| DKK|MasterCard|5643|Withdrawal|null|null| 55.23|11.761|2616038|      Naestved|281.15|1014|  87|   7| 260|0.215|  92| 500|   Rain|          light rain|\n",
      "|2017|January|  1|Sunday|  0|Inactive|  2|NCR|  Vejgaard|         Hadsundvej|  20|9000|57.043|  9.95| DKK|MasterCard|1764|Withdrawal|null|null|57.048| 9.935|2616235|NÃƒÂ¸rresundby|280.64|1020|  93|   9| 250| 0.59|  92| 500|   Rain|          light rain|\n",
      "|2017|January|  1|Sunday|  0|Inactive|  2|NCR|  Vejgaard|         Hadsundvej|  20|9000|57.043|  9.95| DKK|      VISA|1891|Withdrawal|null|null|57.048| 9.935|2616235|NÃƒÂ¸rresundby|280.64|1020|  93|   9| 250| 0.59|  92| 500|   Rain|          light rain|\n",
      "|2017|January|  1|Sunday|  0|Inactive|  3|NCR|     Ikast|RÃƒÂ¥dhusstrÃƒÂ¦det|  12|7430|56.139| 9.154| DKK|      VISA|4166|Withdrawal|null|null|56.139| 9.158|2619426|         Ikast|281.15|1011| 100|   6| 240|  0.0|  75| 300|Drizzle|light intensity d...|\n",
      "|2017|January|  1|Sunday|  0|  Active|  4|NCR|Svogerslev|       BrÃƒÂ¸nsager|   1|4000|55.634|12.018| DKK|MasterCard|5153|Withdrawal|null|null|55.642| 12.08|2614481|      Roskilde|280.61|1014|  87|   7| 260|  0.0|  88| 701|   Mist|                mist|\n",
      "+----+-------+---+------+---+--------+---+---+----------+-------------------+----+----+------+------+----+----------+----+----------+----+----+------+------+-------+--------------+------+----+----+----+----+-----+----+----+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating fileSchema for dataframe\n",
    "fileSchema = StructType([StructField('year', IntegerType(),True),\n",
    "                        StructField('month', StringType(),True),\n",
    "                        StructField('day', IntegerType(),True),\n",
    "                        StructField('weekday', StringType(),True),\n",
    "                        StructField('hour', IntegerType(),True),\n",
    "                        StructField('atm_status', StringType(),True),\n",
    "                        StructField('atm_number', IntegerType(),True),\n",
    "                        StructField('atm_manufacturer', StringType(),True),\n",
    "                        StructField('atm_location', StringType(),True),\n",
    "                        StructField('atm_streetname', StringType(),True),\n",
    "                        StructField('atm_street_number', IntegerType(),True),\n",
    "                        StructField('atm_zipcode', IntegerType(),True),\n",
    "                        StructField('atm_lat', DoubleType(),True),\n",
    "                        StructField('atm_lon', DoubleType(),True),\n",
    "                        StructField('currency', StringType(),True),\n",
    "                        StructField('card_type', StringType(),True),\n",
    "                        StructField('transaction_amount', IntegerType(),True), \n",
    "                        StructField('service', StringType(),True),\n",
    "                        StructField('message_code', StringType(),True),\n",
    "                        StructField('message_text', StringType(),True),\n",
    "                        StructField('weather_lat', DoubleType(),True),\n",
    "                        StructField('weather_lon', DoubleType(),True),\n",
    "                        StructField('weather_city_id', IntegerType(),True),\n",
    "                        StructField('weather_city_name', StringType(),True), \n",
    "                        StructField('temp', DoubleType(),True),\n",
    "                        StructField('pressure', IntegerType(),True),\n",
    "                        StructField('humidity', IntegerType(),True),\n",
    "                        StructField('wind_speed', IntegerType(),True),\n",
    "                        StructField('wind_deg', IntegerType(),True),\n",
    "                        StructField('rain_3h', DoubleType(),True),\n",
    "                        StructField('clouds_all', IntegerType(),True),\n",
    "                        StructField('weather_id', IntegerType(),True), \n",
    "                        StructField('weather_main', StringType(),True),\n",
    "                        StructField('weather_description', StringType(),True)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from hadoop with defined schema\n",
    "df = spark.read.load(\"/user/root/ETL/SRC_ATM_TRANS\", format=\"csv\", sep=\",\", schema = fileSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_number: integer (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: double (nullable = true)\n",
      " |-- atm_lon: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: double (nullable = true)\n",
      " |-- weather_lon: double (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- pressure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: double (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+----------+----------+----------------+------------+--------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+\n",
      "|year|  month|day|weekday|hour|atm_status|atm_number|atm_manufacturer|atm_location|atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|currency| card_type|transaction_amount|   service|message_code|message_text|weather_lat|weather_lon|weather_city_id|weather_city_name|  temp|pressure|humidity|wind_speed|wind_deg|rain_3h|clouds_all|weather_id|weather_main|weather_description|\n",
      "+----+-------+---+-------+----+----------+----------+----------------+------------+--------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+\n",
      "|2017|January|  1| Sunday|   0|    Active|         1|             NCR|  NÃƒÂ¦stved|   Farimagsvej|                8|       4700| 55.233| 11.763|     DKK|MasterCard|              5643|Withdrawal|        null|        null|      55.23|     11.761|        2616038|         Naestved|281.15|    1014|      87|         7|     260|  0.215|        92|       500|        Rain|         light rain|\n",
      "|2017|January|  1| Sunday|   0|  Inactive|         2|             NCR|    Vejgaard|    Hadsundvej|               20|       9000| 57.043|   9.95|     DKK|MasterCard|              1764|Withdrawal|        null|        null|     57.048|      9.935|        2616235|   NÃƒÂ¸rresundby|280.64|    1020|      93|         9|     250|   0.59|        92|       500|        Rain|         light rain|\n",
      "+----+-------+---+-------+----+----------+----------+----------------+------------+--------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2468572|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating temperary table\n",
    "df.registerTempTable(\"ATM_DATA\")\n",
    "\n",
    "# Checking count of dataframe\n",
    "spark.sql(\"select count(*) from ATM_DATA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----------------+-----------+-------+-------+\n",
      "|atm_location|     atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|\n",
      "+------------+-------------------+-----------------+-----------+-------+-------+\n",
      "|  NÃƒÂ¦stved|        Farimagsvej|                8|       4700| 55.233| 11.763|\n",
      "|    Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|\n",
      "|    Vejgaard|         Hadsundvej|               20|       9000| 57.043|   9.95|\n",
      "|       Ikast|RÃƒÂ¥dhusstrÃƒÂ¦det|               12|       7430| 56.139|  9.154|\n",
      "|  Svogerslev|       BrÃƒÂ¸nsager|                1|       4000| 55.634| 12.018|\n",
      "|        Nibe|             Torvet|                1|       9240| 56.983|  9.639|\n",
      "|  Fredericia|   SjÃƒÂ¦llandsgade|               33|       7000| 55.564|  9.757|\n",
      "|   Hjallerup|  Hjallerup Centret|               18|       9320| 57.168| 10.148|\n",
      "| GlyngÃƒÂ¸re|        FÃƒÂ¦rgevej|                1|       7870| 56.762|  8.867|\n",
      "|     Hadsund|          Storegade|               12|       9560| 56.716| 10.114|\n",
      "+------------+-------------------+-----------------+-----------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking location data in dataframe\n",
    "spark.sql(\"select atm_location, atm_streetname, atm_street_number, atm_zipcode, atm_lat, atm_lon from ATM_DATA\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table - Dim Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------------+-----------------+-----------+-------+-------+\n",
      "|location_id|        atm_location|  atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|\n",
      "+-----------+--------------------+----------------+-----------------+-----------+-------+-------+\n",
      "|          1|               Vadum|  Ellehammersvej|               43|       9430| 57.118|  9.861|\n",
      "|          2|            Slagelse| Mariendals Alle|               29|       4200| 55.398| 11.342|\n",
      "|          3|          Fredericia|SjÃƒÂ¦llandsgade|               33|       7000| 55.564|  9.757|\n",
      "|          4|             Kolding|        Vejlevej|              135|       6000| 55.505|  9.457|\n",
      "|          5|   HÃƒÂ¸rning Hallen|        Toftevej|               53|       8362| 56.091| 10.033|\n",
      "|          6|                Aars| Himmerlandsgade|               70|       9600| 56.803|  9.518|\n",
      "|          7|     Aarhus Lufthavn| Ny Lufthavnsvej|               24|       8560| 56.308| 10.627|\n",
      "|          8|                 Fur|      StenÃƒÂ¸re|               19|       7884| 56.805|   9.02|\n",
      "|          9|            Hasseris|     Hasserisvej|              113|       9000| 57.044|  9.898|\n",
      "|         10|Intern  KÃƒÂ¸benhavn|RÃƒÂ¥dhuspladsen|               75|       1550| 55.676| 12.571|\n",
      "|         11|      Skelagervej 15|     Skelagervej|               15|       9000| 57.023|  9.891|\n",
      "|         12|    Intern HolbÃƒÂ¦k|     Slotsvolden|                7|       4300| 55.718| 11.704|\n",
      "|         13|              Viborg|       Toldboden|                3|       8800| 56.448|  9.401|\n",
      "|         14|             SÃƒÂ¦by|      Vestergade|                3|       9300| 57.334| 10.515|\n",
      "|         15|             Aabybro|    ÃƒËœstergade|                6|       9440| 57.162|   9.73|\n",
      "|         16|             Vodskov|      Vodskovvej|               27|       9310| 57.104| 10.027|\n",
      "|         17|               Taars|        Bredgade|               91|       9830| 57.385| 10.116|\n",
      "|         18|         Skive Lobby|        Adelgade|                8|       7800| 56.567|  9.027|\n",
      "|         19|        HelsingÃƒÂ¸r|  Sct. Olai Gade|               39|       3000| 56.036| 12.612|\n",
      "|         20|             Jebjerg|       Kirkegade|                4|       7870| 56.671|  9.013|\n",
      "+-----------+--------------------+----------------+-----------------+-----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating dataframe for dim_location\n",
    "df_dim_location = df.select('atm_location', \n",
    "                            'atm_streetname', \n",
    "                            'atm_street_number', \n",
    "                            'atm_zipcode', \n",
    "                            'atm_lat', \n",
    "                            'atm_lon').distinct().withColumn(\"new_c\",lit(\"ABC\"))\n",
    "df_dim_location = df_dim_location.withColumn(\"location_id\", row_number().over(Window().partitionBy('new_c')\\\n",
    "                                                                         .orderBy(lit('A')))).drop(\"new_c\")\n",
    "df_dim_location = df_dim_location.select('location_id',\n",
    "                                         'atm_location',\n",
    "                                         'atm_streetname',\n",
    "                                         'atm_street_number',\n",
    "                                         'atm_zipcode',\n",
    "                                         'atm_lat',\n",
    "                                         'atm_lon')\n",
    "df_dim_location.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking count of atm location dimension table\n",
    "df_dim_location.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging location dim to original df\n",
    "df = df.join(df_dim_location, on=['atm_location',\n",
    "                                  'atm_streetname',\n",
    "                                  'atm_street_number',\n",
    "                                  'atm_zipcode',\n",
    "                                  'atm_lat',\n",
    "                                  'atm_lon'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the count of distinct location\n",
    "df.select('location_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying total row count\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table - Dim Card Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|card_type_id|           card_type|\n",
      "+------------+--------------------+\n",
      "|           1|     Dankort - on-us|\n",
      "|           2|              CIRRUS|\n",
      "|           3|         HÃƒÂ¦vekort|\n",
      "|           4|                VISA|\n",
      "|           5|  Mastercard - on-us|\n",
      "|           6|             Maestro|\n",
      "|           7|Visa Dankort - on-us|\n",
      "|           8|        Visa Dankort|\n",
      "|           9|            VisaPlus|\n",
      "|          10|          MasterCard|\n",
      "|          11|             Dankort|\n",
      "|          12| HÃƒÂ¦vekort - on-us|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating dataframe for dim_card_type\n",
    "df_dim_card_type = df.select('card_type').distinct().withColumn(\"new_c\",lit(\"ABC\"))\n",
    "df_dim_card_type = df_dim_card_type.withColumn('card_type_id', row_number().over(Window().partitionBy('new_c')\\\n",
    "                                                                         .orderBy(lit('A')))).drop(\"new_c\")\n",
    "df_dim_card_type = df_dim_card_type.select('card_type_id',\n",
    "                                           'card_type')\n",
    "df_dim_card_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging card type dim to original df\n",
    "df = df.join(df_dim_card_type, on=['card_type'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the count of distinct card type\n",
    "df.select('card_type_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying total row count\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table - Dim ATM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+-----------+\n",
      "|atm_id|atm_number|atm_manufacturer|location_id|\n",
      "+------+----------+----------------+-----------+\n",
      "|     1|       109| Diebold Nixdorf|         35|\n",
      "|     2|         3|             NCR|         47|\n",
      "|     3|        82|             NCR|         79|\n",
      "|     4|        95|             NCR|         10|\n",
      "|     5|        95|             NCR|        106|\n",
      "|     6|        85| Diebold Nixdorf|         10|\n",
      "|     7|        85| Diebold Nixdorf|        106|\n",
      "|     8|        63|             NCR|         78|\n",
      "|     9|        41| Diebold Nixdorf|         40|\n",
      "|    10|        41| Diebold Nixdorf|        105|\n",
      "+------+----------+----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating dataframe for dim_atm\n",
    "df_dim_atm = df.select('atm_number',\n",
    "                       'atm_manufacturer',\n",
    "                       'atm_lat',\n",
    "                       'atm_lon').join(df_dim_location, on=['atm_lat',\n",
    "                                                            'atm_lon'], how='left').distinct().withColumn(\"new_c\",lit(\"ABC\"))\n",
    "df_dim_atm = df_dim_atm.withColumn('atm_id', row_number().over(Window().partitionBy('new_c').orderBy(lit('A')))).drop(\"new_c\")\n",
    "df_dim_atm = df_dim_atm.select('atm_id',\n",
    "                               'atm_number',\n",
    "                               'atm_manufacturer',\n",
    "                               'location_id')\n",
    "df_dim_atm.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying count of dim_atm\n",
    "df_dim_atm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To resolve \"resolve(d) attribute(s)\" error\n",
    "df_dim_atm = df_dim_atm.select([col(column_name).alias(column_name) for column_name in df_dim_atm.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merging atm dim to original df\n",
    "df = df.join(df_dim_atm, on=['atm_number', 'atm_manufacturer', 'location_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying total row count\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table - Dim Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----+--------+---+----+---------+\n",
      "|date_id|     full_date_time|year|   month|day|hour|  weekday|\n",
      "+-------+-------------------+----+--------+---+----+---------+\n",
      "|      1|2017-02-27 13:00:00|2017|February| 27|  13|   Monday|\n",
      "|      2|2017-03-22 14:00:00|2017|   March| 22|  14|Wednesday|\n",
      "|      3|2017-04-23 19:00:00|2017|   April| 23|  19|   Sunday|\n",
      "|      4|2017-05-04 18:00:00|2017|     May|  4|  18| Thursday|\n",
      "|      5|2017-08-11 14:00:00|2017|  August| 11|  14|   Friday|\n",
      "|      6|2017-10-07 11:00:00|2017| October|  7|  11| Saturday|\n",
      "|      7|2017-11-14 12:00:00|2017|November| 14|  12|  Tuesday|\n",
      "|      8|2017-04-04 09:00:00|2017|   April|  4|   9|  Tuesday|\n",
      "|      9|2017-06-14 10:00:00|2017|    June| 14|  10|Wednesday|\n",
      "|     10|2017-03-24 08:00:00|2017|   March| 24|   8|   Friday|\n",
      "+-------+-------------------+----+--------+---+----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating dataframe for dim_date\n",
    "df_dim_date = df.select('year', 'month', 'day', 'hour', 'weekday')\n",
    "cols = ['year', 'month', 'day', 'hour']\n",
    "df_dim_date = df_dim_date.withColumn(\"full_date_time\",to_timestamp(concat_ws(\"-\",*cols),\"yyyy-MMM-dd-HH\").cast(\"timestamp\"))\n",
    "df_dim_date = df_dim_date.distinct().withColumn(\"new_c\",lit(\"ABC\"))\n",
    "\n",
    "df_dim_date = df_dim_date.withColumn('date_id', row_number().over(Window().partitionBy('new_c')\\\n",
    "                                                                  .orderBy(lit('A')))).drop(\"new_c\")\n",
    "df_dim_date = df_dim_date.select('date_id',\n",
    "                                 'full_date_time',\n",
    "                                 'year',\n",
    "                                 'month',\n",
    "                                 'day',\n",
    "                                 'hour',\n",
    "                                 'weekday')\n",
    "df_dim_date.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying count of dim_date\n",
    "df_dim_date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merging date dim to original df\n",
    "df = df.join(df_dim_date, on=['year', 'month', 'day', 'hour', 'weekday'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying total row count\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table - Fact ATM Trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating dataframe for fact ATM trans table\n",
    "df = df.withColumn(\"new_c\",lit(\"ABC\"))\n",
    "df = df.withColumn('trans_id', row_number().over(Window().partitionBy('new_c')\\\n",
    "                                                                  .orderBy(lit('A')))).drop(\"new_c\")\n",
    "df_fact_atm_trans = df.select('trans_id','atm_id','location_id','date_id','card_type_id','atm_status','currency',\n",
    "                              'service','transaction_amount','message_code','message_text','rain_3h','clouds_all',\n",
    "                              'weather_id','weather_main','weather_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column from location id to weather_loc_id\n",
    "df_fact_atm_trans = df_fact_atm_trans.withColumnRenamed('location_id','weather_loc_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying count of fact_atm_trans\n",
    "df_fact_atm_trans.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+-------------------+\n",
      "|trans_id|atm_id|weather_loc_id|date_id|card_type_id|atm_status|currency|   service|transaction_amount|message_code|message_text|rain_3h|clouds_all|weather_id|weather_main|weather_description|\n",
      "+--------+------+--------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+-------------------+\n",
      "|       1|    13|           108|     59|           8|    Active|     DKK|Withdrawal|              3999|        null|        null|    0.0|         0|       800|       Clear|       Sky is Clear|\n",
      "|       2|   116|            88|   3922|           1|    Active|     DKK|Withdrawal|              9164|        null|        null|    0.0|        40|       802|      Clouds|   scattered clouds|\n",
      "|       3|    13|           108|     59|           8|    Active|     DKK|Withdrawal|              2251|        null|        null|    0.0|         0|       800|       Clear|       Sky is Clear|\n",
      "|       4|   116|            88|   3922|           1|    Active|     DKK|Withdrawal|              9811|        null|        null|    0.0|        40|       802|      Clouds|   scattered clouds|\n",
      "|       5|    13|           108|     59|          10|    Active|     DKK|Withdrawal|              9533|        null|        null|    0.0|         0|       800|       Clear|       Sky is Clear|\n",
      "+--------+------+--------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_atm_trans.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data to S3\n",
    "\n",
    " - DIM_LOCATION\n",
    " - DIM_ATM\n",
    " - DIM_DATE\n",
    " - DIM_CARD_TYPE\n",
    " - FACT_ATM_TRANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_location to s3\n",
    "df_dim_location.write.option('header','true').csv('s3a://etl-proj/csv/dim_location.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_atm to s3\n",
    "df_dim_atm.write.option('header','true').csv('s3a://etl-proj/csv/dim_atm.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_date to s3\n",
    "df_dim_date.write.option('header','true').option('timestampFormat', 'yyyy-MM-dd HH:mm:ss').csv('s3a://etl-proj/csv/dim_date.csv',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_card_type to s3\n",
    "df_dim_card_type.write.option('header','true').csv('s3a://etl-proj/csv/dim_card_type.csv',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact_atm_trans to s3\n",
    "df_fact_atm_trans.write.option('header','true').csv('s3a://etl-proj/csv/fact_atm_trans.csv',mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
